---
title: Creel Point Estimate Analysis
params:
  proj_name: "District 14"
  water_body: "Cascade River"
  date_start: "2021-09-16"   
  date_end: "2021-11-30"
  time_strata: "month"
  sections: "lut_water_body_location_d14_cascade_fall_salmon.csv"
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
---

This file uses a workflow developed by Dan Auerbach and the creel dev crew to summarize and generate creel estimates (angler effort, catch, and catch per unit effort (cpue)) from freshwater monitoring data housed on data.wa.gov, using deterministic, maximum likelihood estimators (see Kale Bentley's original creel analyis files, Pollock et al. 1994).

Pieces of the script require user / analysis specific inputs, including specification of closure / river out days in the dates section, specification of days with assumed cpue values (when there was known angling effort, but no corresponding interview), and ....? 

# setup 

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(rstan)

#base endpoints
dwg_base <- list(
  event = "https://data.wa.gov/resource/ui95-axtn.csv",
  effort = "https://data.wa.gov/resource/h9a6-g38s.csv",
  interview = "https://data.wa.gov/resource/rpax-ahqm.csv",
  catch = "https://data.wa.gov/resource/6y4e-8ftk.csv",
  gear = "https://data.wa.gov/resource/d2ks-afhz.csv"
)

dwg_sent <- list() #will hold full API strings built on above endpoints with params
creel <- list() #will hold resulting data objects

dates_holidays_2015_2030 <- read_lines("input_files/dates_holidays_2015_2030.txt") |> 
  as.Date(format="%Y-%m-%d")

lut <- map(
  list(
    river_loc = "input_files/lut_River.Locations_2019-01-07.csv",
    creel_models = "input_files/lut_Creel_Models_2021-01-20.csv",
    sections = file.path("input_files", params$sections),
    census_expansion = "input_files/lut_Proportional_Expansions_for_Tie_In_Sections_Cascade_Fall_Salmon_2021.csv"
  ),
  ~readr::read_csv(file.path(.x))
)

# file path for exported objects 

filepath_modeloutput <- "O:/Projects/Creel_Analysis_Dev/CreelPointEstimate/model_output/"

```



# get raw data

The data used are from the `r params$proj_name` project on the `r params$water_body` between `r params$date_start` and `r params$date_end`.

Further development may include interactive control parameter specification via the GUI: [https://bookdown.org/yihui/rmarkdown/params-knit.html#the-interactive-user-interface]

There is also the option to step through multiple pre-defined control parameters:
[https://bookdown.org/yihui/rmarkdown-cookbook/parameterized-reports.html]

## creel events

First, get the creel events of interest by building the Socrata API url string and grabbing the data

```{r get_event}
dwg_sent$event <- URLencode(
  paste0(dwg_base$event,
         "?$where=project_name in('", params$proj_name, "')",
         " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
         " AND event_date between '", params$date_start,
         "T00:00:00' and '", params$date_end,
         "T00:00:00'&$limit=100000"
  )
)

creel$event <- read_csv(dwg_sent$event) |> 
  dplyr::select(creel_event_id, water_body, event_date, tie_in_indicator)

```

Then, get the associated effort and interview data.

```{r pending_vw_changes_for_water_body}
# #if water_body dropped from event filter...
# #but regardless can/should build creel_event_id condition once and apply twice?
# eff_int_filter <- paste0(
#     "?$where=creel_event_id in('",
#     paste(creel$event$creel_event_id, collapse = "','"), "')",
#     " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
#     "&$limit=100000"
#   )

```

## effort counts

```{r get_effort}
dwg_sent$effort <- URLencode(
  paste0(dwg_base$effort,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$effort <- read_csv(dwg_sent$effort)

```

## interviews

```{r get_interview}
dwg_sent$interview <- URLencode(
  paste0(dwg_base$interview,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$interview <- read_csv(dwg_sent$interview) |> 
  rename(location = interview_location)

```

## catch data

And finally, the catch data associated with the interviews.

```{r get_catch}
dwg_sent$catch <- URLencode(
  paste0(dwg_base$catch,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)


creel$catch <- read_csv(dwg_sent$catch) 

creel$catch <- creel$catch |> 
  dplyr::select(interview_id, catch_id, species, run, life_stage, fin_mark, fate, fish_count) |> 
  mutate(
    life_stage = replace_na(life_stage, "Adult"), # placeholder pending data corrections in fish apps 
    catch_group = paste(species, life_stage, fin_mark, fate, sep = "_") # fish catch groups to estimate catch of 
  )
  
```



## sections

Aggregations of `location` units that depend on `r params$section` lookup table.

**NOTE ANY DATA THAT ARE NOT ASSIGNED TO A SECTION WILL BE EXCLUDED**

```{r add_sections}
creel$effort <- creel$effort |> 
  select(-created_datetime, -modified_datetime) |>
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, section),
    by = c("location")
    ) |> 
  filter(!is.na(section))

creel$interview <- creel$interview |> 
  select(-created_datetime, -modified_datetime,
         -state_residence, -zip_code) |> 
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, section),
    by = c("location")
  ) |> 
  filter(!is.na(section))

```



# prepare data for catch and effort calculations

```{r init_point_estimate_data_prelim}
#declare an intermediary list

point_estimate_data_prelim <- list(
  effort_census = creel$effort |> filter(tie_in_indicator == 1),
  effort_index = creel$effort |> filter(tie_in_indicator == 0),
  interview = creel$interview
)

```


## dates

First, build an "expanded dates lattice" to which any/all observations are attached, ensuring complete cases.

Start with presumption that all dates are open, then join per-section closures by date.

```{r d_days}
#creel$event |> distinct(event_date)
d_days <- tibble(
  event_date = seq(
    as.Date(params$date_start, "%Y-%m-%d"),
    as.Date(params$date_end, "%Y-%m-%d"),
    by = "day")
) |> 
  mutate(
    Day = weekdays(event_date),
    DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% dates_holidays_2015_2030, "Weekend", "Weekday"),
    DayType_num = if_else(str_detect(DayType, "end"),1,0),
    DayL = suncalc::getSunlightTimes(
      date = event_date,
      tz = "America/Los_Angeles",
      #need to add flexibility for other rivers/multiple lines in River.Locations lut
      lat = lut$river_loc$Lat,
      lon = lut$river_loc$Long,
      keep=c("dawn", "dusk")
    ) |> 
      mutate(DayL = as.numeric(dusk - dawn)) |>
      pluck("DayL"),
    Week = as.numeric(format(event_date, "%V")),
    Month = as.numeric(format(event_date, "%m"))) |> 
  rowid_to_column(var = "day_index") |> 
  #make open section cols (only those actually used, not all in LU)
  left_join(
    expand_grid(
      event_date = seq(
        as.Date(params$date_start, "%Y-%m-%d"),
        as.Date(params$date_end, "%Y-%m-%d"),
        by = "day")
      ,
      s = paste0("open_section_", sort(unique(point_estimate_data_prelim$effort_index$section)))
    ) |> 
      mutate(closure_code = TRUE) |> 
      pivot_wider(names_from = s, values_from = closure_code)
    ,
    by = "event_date")

# #now add closures if any...
# #commented out during dev around Skagit_wgf, since no closures
# #and because may move to params file path to closures csv or similar

d_days <- rows_upsert(d_days,
    bind_rows(
      tibble(section = "1", closure_begin = "2021-09-18", closure_end = "2021-09-18"), # river out due to flows 
      tibble(section = "1", closure_begin = "2021-09-19", closure_end = "2021-09-20"), # treaty fishery closure
      tibble(section = "1", closure_begin = "2021-09-26", closure_end = "2021-09-27"), # treaty fishery closure
      tibble(section = "1", closure_begin = "2021-10-03", closure_end = "2021-10-04"), # treaty fishery closure
      tibble(section = "1", closure_begin = "2021-11-13", closure_end = "2021-11-14"), # river out due to flows
      tibble(section = "1", closure_begin = "2021-11-17", closure_end = "2021-11-18") # river out due to flows
    ) |>
      rowwise() |>
      mutate(closure_date = paste(seq.Date(as.Date(closure_begin), as.Date(closure_end), by = "day"), collapse = ",")) |>
      separate_rows(closure_date, sep = ",") |>
      select(event_date = closure_date, section) |>
      mutate(
        event_date = as.Date(event_date),
        closure_code = FALSE # TB - The 1e-06 is needed to keep the model from crashing ?log-normal parameters cant be 0
      ) |>
      separate_rows(section, sep = ",") |>
      pivot_wider(names_from = section, names_prefix = "open_section_", values_from = closure_code) |>
      mutate(across(starts_with("open_section_"), ~replace_na(., TRUE)))
    ,
    by ="event_date"
  )


```

## evaluate completeness of index counts / interviews 

```{r count and interview evaluation}

data_eval <- list()

# Visualize raw counts of vehicles and trailers during monitoring period
# are periods with virtually 0 effort worth running through the model?
data_eval$index_counts <-  creel$effort |>
  filter(tie_in_indicator == 0) |>
  group_by(section, event_date, count_sequence, count_type) |>
  summarise(count_quantity = sum(count_quantity), .groups = "drop") |>
  pivot_wider(names_from = count_type, values_from = count_quantity) |>
  # select(-6) |>
  # mutate(
  #   `Trailers Only` = replace_na(`Trailers Only`, 0)
  # ) |>
  pivot_longer(names_to = "count_type", values_to = "count_quantity", cols = c(`Vehicle Only`)) |>
  arrange(event_date, section)

# data_eval$index_counts |>
# ggplot(aes(x = event_date, y = count_quantity)) +
#   geom_point(aes(fill=count_type), colour="black",pch=21, size=2) +
#   scale_fill_manual(values=c("orange", "blue")) +
#   theme_bw() +
#   facet_wrap(.~ section)


# Assess data completeness by looking for deviations from the expected count of distinct effort count locations per section per day. Note - days can look complete if "no count" has been entered for a site, so this is just one view into potential data completeness issues
data_eval$location_check <- creel$effort |>
  filter(tie_in_indicator == 0) |>
  select(event_date, section, location, count_sequence) |>
  group_by(event_date, section, count_sequence) |>
  summarise(
    n_distinct_locations =  n_distinct(location),
    distinct_locations = paste(unique(location), collapse = ","))

# data_eval$location_check |>  ggplot(aes(x = event_date, y = n_distinct_locations)) +
#   geom_point() +
#   theme_bw() +
#   facet_wrap(.~ section)

# Look at records where "no_count_reason" was used
data_eval$no_count_records <- creel$effort |>
  filter(tie_in_indicator == 0, !is.na(no_count_reason)) |>
  arrange(event_date)

# Evaluate on a daily basis the number of boat and bank interviews obtained
data_eval$interview <- creel$interview |>
  mutate(
    interview_type = if_else(boat_used == "Yes", "Boat", "Bank")
  ) |>
  group_by(event_date, section, location, interview_type) |>
  summarize(
    n_interviews = n()
  ) |>
  pivot_wider(names_from = interview_type, values_from = n_interviews, values_fill = 0) |>
  pivot_longer(names_to = "interview_type", values_to = "n_interviews", cols = starts_with("B")) |>
  arrange(event_date, section)

# data_eval$interview |>
# ggplot(aes(x = event_date, y = n_interviews)) +
#   geom_point(aes(fill=interview_type), colour="black",pch=21, size=2) +
#   scale_fill_manual(values=c("orange", "blue")) +
#   theme_bw() +
#   facet_wrap(.~ section)

```

## fishery specific data issues 
```{r data clean up}

# chunk for fishery specific data QA/QC issues

# Skagit issues which may or may not occur elsewhere include missing locations from drive around index effort counts and a location that was supposed to be monitored, but wasn't, for over half of the season (Skagit City Access). Would be good see data issues from other creels to think about what's general vs. project specific. 

# effort count location that was left out of counts for large chunk of season

#  EB commented out 2/22/2022 pending further review of Cascade River data 

data_eval$remove_location <- bind_rows(
  tibble(location = ")"))

# mistakes in data collection led to instances of incomplete effort counts, this chunk provides the user an option to censor those data by filtering them out of the final dataset
# count_id is a paste of event_date, section, count_sequence

creel$effort <- creel$effort |>
   mutate(
    count_id = paste(event_date, section, count_sequence, sep = "_")
  )

# declare individual count_sequences which contain missing counts and should be filtered from final dataset

data_eval$remove_count <- bind_rows(
    tibble(count_id = "")
    )

# # n rows in effort data pre filter
data_eval$counts_pre_filter <- nrow(creel$effort)

creel$effort <- creel$effort |>
  filter(!location %in% data_eval$remove_location$location, !count_id %in% data_eval$remove_count$count_id) |>
  arrange(event_date, section, count_sequence)

# # n rows in effort data post filter
data_eval$counts_post_filter <- nrow(creel$effort)

# number of counts removed from dataset
print(data_eval$counts_pre_filter - data_eval$counts_post_filter)

# Placeholder as option to replace NA's with 0's if/when deemed appropriate
creel$effort <- creel$effort |>
  mutate(count_quantity = if_else(!is.na(no_count_reason), 0, count_quantity))


```



## effort data 

### effort census

Aggregate census (tie in) effort counts, associating to closest-in-time index count. 

```{r point_estimate_data_prelim_effort_census}
#to the initial effort_census, with all count_sequence == 1,
#add/overwrite the count_sequence val with that from closest temporal match from inline/anonymous paired counts object 

point_estimate_data_prelim$effort_census <- point_estimate_data_prelim$effort_census |> 
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_type, count_quantity) |> 
  left_join(
    left_join(
      point_estimate_data_prelim$effort_census |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      point_estimate_data_prelim$effort_index |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      by = c("event_date", "section"),
      suffix = c("_cen", "_ind")
      ) |> 
      group_by(event_date, section, location_cen) |> 
      slice_min(abs(effort_start_time_cen - effort_start_time_ind), n = 1) |> 
      ungroup() |> 
      #count(event_date, section, location_cen, count_sequence_cen, count_sequence_ind)
      distinct(event_date, section, location = location_cen, count_sequence = count_sequence_ind)
    ,
    by = c("event_date", "section", "location")
  ) |> 
  left_join(d_days, by = "event_date") |>
  mutate(
    #angler_type = word(count_type, 1),
    angler_type = case_when(
      word(count_type, 1) %in% c("Bank","Shore") ~ "bank_ang",
      word(count_type, 1) %in% c("Boat") ~ "boat_ang"
    )
  ) |>
  #exclude any count_type strings we didn't whitelist into an angler_type
  #e.g., "Boats" which are not a thing we use because reasons
  filter(!is.na(angler_type)) |> 
  group_by(event_date, day_index, section, count_sequence, angler_type) |>
  summarize(count_quantity = sum(count_quantity), .groups = "drop") |>
  mutate(tie_in_indicator = 1) |> 
  arrange(event_date, section, count_sequence) 


```

### effort index

Aggregate index counts of vehicles, trailers, anglers, and boats.

```{r point_estimate_data_prelim_effort_index}

##DA added filter(!is.na(count_quantity)) to address above comments...
## need to think about whether !is.na(no_count_reason) should be interpolated or inferred or zeroed or...
## for example no_count_reason == "Conditions", what to do with count_quantity == NA
## Skagit winter gamefish example also shows valid realworld situation
## section 1 has 'Vehicle Only' counts but not 'Trailers Only' due to low angler effort
## so excluding section 1 is legit, but needs to be clearly indicated...
point_estimate_data_prelim$effort_index <- point_estimate_data_prelim$effort_index |> 
  filter(
    #!is.na(count_quantity)
    is.na(no_count_reason),
    !is.na(count_type)
    ) |>  
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_sequence, count_type, count_quantity) |>
  left_join(d_days, by = "event_date") |> 
  group_by(section, event_date, day_index, Week, Month, count_sequence, count_type) |> 
  summarise(count_quantity = sum(count_quantity), .groups = "drop") |> 
  # pivot_wider(names_from = count_type, values_from = count_quantity) |> 
  arrange(event_date, section, count_sequence) 

```


### effort totals per count_sequence and mean daily values of vehicle and trailer counts 
```{r}
# average index effort count by day, angler_type, and section, which is the mean value of the sum total of effort counts conducted during n number of count_sequence within a day 

# sum total of vehicle trailer counts per date, section, sampling event (count_sequence), and count_type (vehicles, trailers, etc.)
point_estimate_data_prelim$Daily_effort_per_count_index_counts <- point_estimate_data_prelim$effort_index |> 
  group_by(event_date, section, count_sequence, count_type) |> 
  summarize(
    sum_index_count = sum(count_quantity)
  ) 

# Mean count across multiple sampling events within a day 

point_estimate_data_prelim$mean_daily_effort_index_counts <- point_estimate_data_prelim$Daily_effort_per_count_index_counts|> 
  group_by(event_date, section, count_type) |> 
  summarise(
    mean_index_count = mean(sum_index_count)
  ) |> 
  mutate(
    angler_type = if_else(count_type == "Trailers Only", "boat", "total")
  ) |> 
  mutate(
    angler_type = if_else(angler_type == "total", "bank", "boat") # Cascade only - all anglers are bank anglers so coercing "total" to "bank" to match grouping in interview data 
  )

```


## interview data 

```{r point_estimate_data_prelim_interview}

# derive needed fields from interview data and join fish catch data 
point_estimate_data_prelim$interview <- point_estimate_data_prelim$interview |> 
  left_join(d_days, by = "event_date") |> #summary()
  mutate(
    across(c(vehicle_count, trailer_count), ~replace_na(., 0)),
    trip_status = replace_na(trip_status, "Unknown"),
    angler_type = case_when(
      is.na(fish_from_boat) ~ "bank",
      fish_from_boat == "Bank" ~ "bank",
      fish_from_boat == "Boat" ~ "boat"
      ),
    angler_type_ind = as.integer(factor(angler_type)),
    fishing_end_time = if_else(is.na(fishing_end_time), interview_time, fishing_end_time),
    angler_hours = round(as.numeric(fishing_end_time - fishing_start_time) / 3600, 5),
    angler_hours_total = angler_count * angler_hours
  ) |> 
  left_join(
    creel$catch |>
      mutate(
    life_stage = replace_na(life_stage, "Adult"),
    catch_group = paste(species, life_stage, fin_mark, fate, sep = "_")
  ) |> 
    group_by(interview_id, catch_group) |> 
    summarise(fish_count = sum(fish_count), .groups = "drop")
    ,by = "interview_id"
  ) |> 
  select(section, event_date, day_index, DayL, DayType, Week, Month, interview_id, interview_time, contains("angler"), catch_group, contains("count")) |>
  mutate(across(fish_count, ~replace_na(., 0)),
         catch_group = replace_na(catch_group, "no_catch")) |> 
  filter(angler_hours > 0.5)

# obtain unique catch_groups present in intermediate interview object 

point_estimate_data_prelim$species_present <- point_estimate_data_prelim$interview |> 
  distinct(catch_group)

# split off fish count data to pivot out on catch groups to extend catch group information to each interview (essentially inferring 0's in interviews when catch for a group is not reported)

point_estimate_data_prelim$catch_data_pivot_wide <- point_estimate_data_prelim$interview |> 
  select(interview_id, catch_group, fish_count) |> 
  pivot_wider(names_from = catch_group, values_from = fish_count, values_fill = 0) |> 
  select(interview_id, contains(point_estimate_data_prelim$species_present$catch_group)) |>
  group_by(interview_id) |> 
  summarise(across(contains(point_estimate_data_prelim$species_present$catch_group), sum))

# rejoin catch data to interview data and pivot long on counts per catch group

point_estimate_data_prelim$interview <- point_estimate_data_prelim$interview |> 
  select(-c(catch_group, fish_count)) |>
  distinct(interview_id, .keep_all = TRUE) |> 
  left_join(point_estimate_data_prelim$catch_data_pivot_wide, by = "interview_id") |>
  pivot_longer(cols = contains(point_estimate_data_prelim$species_present$catch_group), names_to = "catch_group", values_to = "fish_count")

```



# evaluate days with measured effort by no interview

```{r stan_data_prelim_interview_daily_totals}
# total hours from angler interviews and total catch sample a per day/angler type/section to compare with effort

point_estimate_data_prelim$interview_daily_totals <- point_estimate_data_prelim$interview |>
  group_by(event_date, day_index, Week, Month, section, angler_type) |>
  summarise(
    angler_hours_total_dailysum = sum(angler_hours_total),
    catch_dailysum = sum(fish_count), .groups = "drop"
  ) |> 
  select(event_date, section, angler_type, angler_hours_total_dailysum, catch_dailysum)

effort_no_interview <- point_estimate_data_prelim$mean_daily_effort |> 
  left_join(point_estimate_data_prelim$interview_daily_totals, by = c("event_date", "section", "angler_type"))

# days with effort but not interview: 11/19, 11/28, 11/29
# these are days we need to "assume" CPUE / hpue using something standardized, consistent, and reproducible

```



# daily estimates of effort 

Here we seek to calculate estimates of daily effort (angler-hours) from vehicle / trailer counts (index counts) and angler group data (interviews), following methods used in North Puget Sound creels for a number of years. Methods outlined in Pollock and Hahn (I think) use actual counts of anglers. North sound creels use counts of 1) vehicles and 2) trailers as surrogates for 1) total number of anglers and 2) a subset of anglers fishing from boats. The use of vehicle and trailer counts requires additional information to translate the index counts into estimates of anglers. Here I've adopted the approach used in Excel workbooks, which uses interview information to calculate rates of anglers per vehicle / trailer. The angler per index count is then multiplied by the mean of sum total of the index counts for a given day and section. 

An exception to this is the Cascade River, where only vehicles get counted, since there's no boat access on the 1 mile long fishery. 

## join daily angler data from interviews to daily summaries of index counts 
```{r}

# summarize interview data by section, angler-type (boat, bank), and event_date

point_estimate_data_prelim$angler_data_from_interviews <- point_estimate_data_prelim$interview |> 
  group_by(section, angler_type, event_date, day_index, DayL, Week, Month) |> 
  summarize(
    daily_sum_angler = sum(angler_count),
    daily_sum_index_count_from_interview = sum(vehicle_count),
    angler_hours_total = sum(angler_hours_total),
    anglers_per_index_count_from_interview = daily_sum_angler / daily_sum_index_count_from_interview)
    

# several days where boat effort estimate from trailers exceed total estimate from vehicles, could be sampling bias that's underrepresenting bank anglers hours / overrepresenting boat angler hours in interviews

# Circle back to this on Skagit River data 

# Hahn manual - if no interview for given strata, treat effort counts as missing data (unsampled day), and use assumed value of CPUE in calculation of mean CPUE   

# this left join is slicing off index counts with no corresponding interview
point_estimate_data_prelim$effort_counts_and_interviews <- point_estimate_data_prelim$angler_data_from_interviews  |> 
  left_join(point_estimate_data_prelim$mean_daily_effort_index_counts, by = c("event_date", "section", "angler_type")) |> 
    mutate(
    mean_daily_effort = DayL * anglers_per_index_count_from_interview * mean_index_count) |>
  arrange(event_date, section)

```




## join census count data to index count data and apply bias term ratio to unexpanded index counts 

```{r}

# calculate mean vehicle and trailer counts and corresponding anglers per vehicle / anglers per trailer by day, section, and count sequence to match temporal scale of census counts 



# EB 2/25/2022 this will need to be redone for script dealing with bank and boat effort, since boat effort will be a subset of the total effort derived from vehicle counts...
# though is this necessary if we're matching up vehicle(s) to trailers and then removing those vehicles from the total pool of vehicles counted? Could be hard to accurately assess the # of vehicles per boat though, lots of weird caveats / situations. 

# assign index counts to either boat or bank effort 

point_estimate_data_prelim$Daily_effort_per_count_groups <- point_estimate_data_prelim$Daily_effort_per_count |>
  mutate(
    angler_type = if_else(count_type == "Trailers Only", "boat", "total")
  ) |> 
  select(-count_type) |> 
  mutate(angler_type = if_else(angler_type == "total", "bank", "boat")) # Cascade fix
  
# index count derived estimates of angler counts

point_estimate_data_prelim$effort_counts_for_census_join <- point_estimate_data_prelim$angler_data_from_interviews |> 
  left_join(point_estimate_data_prelim$Daily_effort_per_count_groups, by = c("event_date", "section", "angler_type")) |> 
  mutate(
    index_count = anglers_per_index_count_from_interview * sum_index_count
  ) |> 
  ungroup() |> 
  select(event_date, section, Week, Month, DayL, count_sequence, angler_type, index_count)
  
# left join effort counts to census counts 

point_estimate_data_prelim$census_counts_total <- point_estimate_data_prelim$effort_census |> 
  group_by(angler_type, event_date, section, day_index, count_sequence) |> 
  summarize(
    count_quantity = sum(count_quantity)
  ) |> 
  mutate(
    angler_type = case_when(
      angler_type == "bank_ang" ~ "bank",
      angler_type == "boat_ang" ~ "boat"
    )
  )

# left in for when I test this on Skagit data with vehicles and trailers (though I think no needed when grouping by angler_type) 

# point_estimate_data_prelim$census_counts_boat <- point_estimate_data_prelim$effort_census |>
#   filter(angler_type == "boat_ang") |> 
#   group_by(event_date, section, day_index, count_sequence) |> 
#   summarize(
#     count_quantity = sum(count_quantity)
#   ) |> 
#   mutate(
#     angler_type = "boat"
#   )

# filter out tie in ratios == NaN or Inf 
 
# calculate bias term ratios (ratio of census counts to angler count estimates from index counts)

point_estimate_data_prelim$census_counts_all <- point_estimate_data_prelim$census_counts_total |> 
  bind_rows(point_estimate_data_prelim$census_counts_boat) |> 
  select(
    event_date, section, count_sequence, 
    census_count = count_quantity, angler_type
  ) |> 
  left_join(point_estimate_data_prelim$effort_counts_for_census_join, by = c("event_date", "section", "count_sequence","angler_type")) |>
  mutate(
    index_count = replace_na(index_count, 0), #EB placeholder,
    TI_Expan = census_count / index_count) |> 
  filter(
    !is.na(TI_Expan), !is.infinite(TI_Expan)
  ) |> 
  select(
    event_date, day_index, Week, Month, DayL, section, DayL, angler_type, census_count, index_count, TI_Expan
  ) |> 
  group_by(section, angler_type) |> 
  summarise(
    TI_Expan_Total = sum(census_count) / sum(index_count))

# days object selected down to just what's needed in join between index and census counts 

days_join <- d_days |> 
  select(event_date, DayType, Month, Week, open_section_1)

# join census (tie in) counts to index counts 

point_estimate_data_prelim$effort_interviews_final <- point_estimate_data_prelim$effort_counts_and_interviews |>
  ungroup() |> 
  left_join(point_estimate_data_prelim$census_counts_all, by = c("section", "angler_type")) |> 
  select(-c(Month, Week)) |> 
  mutate(
    mean_daily_TI_Expan = mean_daily_effort * TI_Expan_Total
  ) |> 
  right_join(days_join, by = "event_date") |> 
  mutate(
    creeled = if_else(!is.na(mean_daily_TI_Expan), "Y", "N")
  ) |>
  arrange(event_date)


```

## calculate number of sampled and unsampled days per day type and time period strata, add to daily effort-interview dataset  

```{r}

# total number days within monitoring period that we want to generate estimates for

total_days <- point_estimate_data_prelim$effort_interviews_final |>
  filter(open_section_1 == TRUE) |> 
group_by(DayType, Month) |> 
  summarize(
    N_days = n()
  )

# number of days sampled within monitoring period

sampled_days <- point_estimate_data_prelim$effort_interviews_final |>
  filter(!is.na(mean_daily_effort)) |> 
group_by(DayType, Month) |> 
  summarize(
    n_days = n()
  )

# add these objects to the table with index count data 

point_estimate_data_prelim$effort_interviews_final <- point_estimate_data_prelim$effort_interviews_final |> 
  left_join(total_days, by = c("DayType", "Month"))

# add these objects to the table with index count data 

point_estimate_data_prelim$effort_interviews_final <- point_estimate_data_prelim$effort_interviews_final |> 
  left_join(sampled_days, by = c("DayType", "Month")) |> 
  mutate(
    n_days = replace_na(n_days, 0),
    degfree = median(as.vector(c(1,25))) # EB need to code dynamic way to derive d.f. based on creel manual 
  ) |>
  filter(!is.na(mean_daily_TI_Expan)) 

# now, need daily CPUE on sampled days, and need to identify days where effort was sampled but no corresponding interviewed was obtained to estimate CPUE

```


# daily catch per unit effort (CPUE) from interviews 
```{r daily CPUE from interviews}


# KB 2017: H better estimate would be the “mean-of-ratios” CPUE (i.e., average CPUE per group divided by number of groups interviewed), which alleviates the potential for a “length-of-stay” bias caused by the inclusion of both complete and incomplete trip data in the estimate (Pollock et al. 1994). Length of stay bias occurs when anglers who fish longer are more likely to be interviewed and their resulting information is unequally weighted when calculating CPUE.


# pivot wide to get catch across all catch groups 
# pivot long to make calculations on fish_count (cpue)

point_estimate_data_prelim$interview_daily_totals <- point_estimate_data_prelim$interview |>
   mutate(
    cpue_interview = fish_count / angler_hours_total 
  ) |> 
  group_by(section, Month, DayType, event_date, day_index, angler_type, catch_group) |> 
  summarise(
    angler_hours_total_dailysum = sum(angler_hours_total),
    catch_dailysum = sum(fish_count), .groups = "drop",
    cpue_mor = mean(cpue_interview),  # mean of ratios CPUE estimator 
    cpue_var = var(cpue_interview)
  ) |> 
  mutate(
    cpue_rom = catch_dailysum / angler_hours_total_dailysum # ratio of the means CPUE estimator 
  )
  
# calculate mean cpue values by DayType, Month, Catch_Group to use for assumed values 

point_estimate_data_prelim$cpue_summary <- point_estimate_data_prelim$interview_daily_totals |> 
  group_by(section, Month, DayType, angler_type, catch_group) |> 
  summarise(
    cpue_rom = mean(cpue_rom) # EB using ratio of the means for now but should circle back to this soon
  )

# Using a rows upsert approach to stuff in CPUE for days with known effort but no interview. This spirals out of control with additional catch groups and should probably be done using a for loop or apply framework 

point_estimate_data_prelim$interview_daily_totals <- rows_upsert(point_estimate_data_prelim$interview_daily_totals,
  bind_rows(
    # coho catch groups 
      tibble(event_date = "2021-11-19",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_AD_Kept")),
      tibble(event_date = "2021-11-28", 
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekday", catch_group == "Coho_Adult_AD_Kept")),
      tibble(event_date = "2021-11-29",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_AD_Kept")),
      tibble(event_date = "2021-11-19",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_AD_Released")),
      tibble(event_date = "2021-11-28",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekday", catch_group == "Coho_Adult_AD_Released")),
      tibble(event_date = "2021-11-29",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_AD_Released")),
      tibble(event_date = "2021-11-19",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_UM_Kept")),
      tibble(event_date = "2021-11-28",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekday", catch_group == "Coho_Adult_UM_Kept")),
      tibble(event_date = "2021-11-29",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_UM_Kept")),
      tibble(event_date = "2021-11-19",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_UM_Released")),
      tibble(event_date = "2021-11-28",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekday", catch_group == "Coho_Adult_UM_Released")),
      tibble(event_date = "2021-11-29",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Coho_Adult_UM_Released")),
    
    # Chinook catch groups   
      tibble(event_date = "2021-11-19",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Chinook_Adult_AD_Released")),
      tibble(event_date = "2021-11-28",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekday", catch_group == "Chinook_Adult_AD_Released")),
      tibble(event_date = "2021-11-29",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Chinook_Adult_AD_Released")),
      tibble(event_date = "2021-11-19",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Chinook_Adult_UM_Released")),
      tibble(event_date = "2021-11-28",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekday", catch_group == "Chinook_Adult_UM_Released")),
      tibble(event_date = "2021-11-29",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Chinook_Adult_UM_Released")),
      tibble(event_date = "2021-11-19",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Chinook_Adult_UNK_Released")),
      tibble(event_date = "2021-11-28",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekday", catch_group == "Chinook_Adult_UNK_Released")),
      tibble(event_date = "2021-11-29",  
             point_estimate_data_prelim$cpue_summary |> filter(section == 1, Month == 11, angler_type == "bank", 
                                    DayType == "Weekend", catch_group == "Chinook_Adult_UNK_Released"))
      ) |>
    rowwise() |>
      mutate(event_date = as.Date(event_date, "%Y-%m-%d"))
    , by = c("event_date", "catch_group"))

# phew! that's gross! 

# extract the columns we need from the interview and catch daily total table to calculate daily CPUE

point_estimate_data_prelim$daily_cpue <- point_estimate_data_prelim$interview_daily_totals |> 
  select(section, event_date, angler_type, catch_group, cpue_rom) |> 
  filter()


```



# daily catch (effort * CPUE) estimates 
```{r daily catch estimates}

# empty list for summarized estimates we want to export

estimates <- list()

# join tables with daily effort (angler hours) and daily catch rate (cpue)

estimates$daily_effort_cpue_catch <- point_estimate_data_prelim$effort_interviews_final |> 
  left_join(point_estimate_data_prelim$daily_cpue, by = c("section", "event_date", "angler_type")) |> 
  mutate(
    catch_estimate = mean_daily_TI_Expan * cpue_rom
  ) |> 
  mutate_if(is.numeric, round, 2) |> 
    filter(str_detect(catch_group, 'Coho|Chinook')) |>  # placeholder filter 
  arrange(catch_group, section, event_date, angler_type)

write_csv(estimates$daily_effort_cpue_catch, paste(filepath_modeloutput, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Daily_Effort_Catch_CPUE", sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))
          
```



# time period specific estimates 

Need to work on making the model period a script parameter so the user can easily swap out time period strata among week, month, bi-month, etc. 

## effort by time period strata 
```{r}

# calculate effort by day type and angler type strata

estimates$monthly_effort_by_daytype <- estimates$daily_effort_cpue_catch |>
  group_by(section, angler_type, Month, DayType, n_days, N_days) |> 
  summarize(
    Sum_daily_effort_sampled_days = sum(mean_daily_TI_Expan),
    Mean_daily_effort = mean(mean_daily_TI_Expan),
    Variance_daily_effort = var(mean_daily_TI_Expan),
    Total_effort = Mean_daily_effort * N_days,
    Variance_total_daily_effort = (N_days^2) * (Variance_daily_effort / n_days) * (1-(n_days/N_days))
  ) |> 
  distinct()

# Sum effort estimates across day types, retain grouping on angler type 

estimates$monthly_effort_total <- estimates$monthly_effort_by_daytype |>
  ungroup() |> 
  group_by(section, Month, angler_type) |> 
  summarise(
    Total_effort = sum(Total_effort),
    Variance = sum(Variance_total_daily_effort),
    SE = sqrt(Variance),
    CV = SE / Total_effort,
    lwr90CI = Total_effort - qt(1-(0.1/2),estimates$daily_effort_cpue_catch$degfree)*(Variance^0.5),
    upr90CI = Total_effort + qt(1-(0.1/2),estimates$daily_effort_cpue_catch$degfree)*(Variance^0.5)
  ) |> 
  distinct(Month, angler_type, .keep_all = TRUE)|> 
  mutate_if(is.numeric, round, 2)

view(estimates$monthly_effort_total)

write_csv(estimates$monthly_effort_total, paste(filepath_modeloutput, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Monthly_Effort", sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))

```


## CPUE by time period strata  
```{r}

# EB circle back to this for calculating CPUE / HPUE and associated uncertainty over time period of interest 

# point_estimate_data_prelim$total_days_per_month <- point_estimate_data_prelim$interview_daily_totals |> 
#   select(Month, N_days) |>
#   distinct() |> 
#   group_by(Month) |> 
#   summarize(
#     N_total_days = sum((N_days))
#   )
# 
# estimates$interview_cpue_monthly_by_daytype <- point_estimate_data_prelim$interview_daily_totals |>
#   ungroup() |> 
#   left_join(point_estimate_data_prelim$total_days_per_month, by = "Month") |>
#   mutate(
#     degfree = min(n_days) - 1) |> 
#   group_by(section, Month, angler_type) |> 
#   summarize(
#     ratio_daytype1 = n() / N_total_days,
#     ratio_daytype2 = n_days / N_total_days,
#     mean_cpue = round(mean(cpue_rom), 4),
#     cpue_ratio_daytype = ratio_daytype1 * mean_cpue,
#     variance_daily_cpue = var(cpue_rom),
#     SE = sqrt(variance_daily_cpue),
#     CV = SE / mean_cpue,
#     lwr90CI = mean_cpue - qt(1-(0.1/2),degfree)*(variance_daily_cpue^0.5),
#     upr90CI = mean_cpue + qt(1-(0.1/2),degfree)*(variance_daily_cpue^0.5)) |> 
#   filter(!is.na(Month)) |> 
#   distinct()
# 

```



## catch by time period strata 
```{r}

# calculate catch by day type and angler type strata

estimates$monthly_catch_by_daytype <- estimates$daily_effort_cpue_catch |>
  group_by(section, angler_type, catch_group, Month, DayType, n_days, N_days) |> 
  summarize(
    total_catch_unexpanded = sum(catch_estimate),
    mean_daily_catch = total_catch_unexpanded / n_days,
    total_catch_expanded = mean_daily_catch * N_days,
    variance_catch_estimate = var(catch_estimate),
    n_days = mean(n_days),
    N_days = mean(N_days),
    variance_total_catch_expanded = (N_days^2)*(variance_catch_estimate/n_days)*(1-(n_days/N_days))
  ) |> 
  distinct()


# Sum catch estimates across day types, retain grouping on angler type 

estimates$monthly_catch_total <- estimates$monthly_catch_by_daytype |>
  ungroup() |> 
  group_by(section, Month, catch_group, angler_type) |> 
  summarise(
    total_catch = sum(total_catch_expanded),
    variance = sum(variance_total_catch_expanded),
    SE = sqrt(variance),
    CV = SE / total_catch,
    lwr95CI = total_catch - qt(1-(0.05/2),estimates$daily_effort_cpue_catch$degfree)*(variance^0.5),
    upr95CI = total_catch + qt(1-(0.05/2),estimates$daily_effort_cpue_catch$degfree)*(variance^0.5)
  ) |>
  distinct() |> 
  mutate_if(is.numeric, round, 2) |> 
  arrange(section, catch_group, Month)

view(estimates$monthly_catch_total)

write_csv(estimates$monthly_catch_total, paste(filepath_modeloutput, paste(paste(params$proj_name, params$water_body, sep = "_"), paste("Point_Estimate", "Summary_Monthly_Catch", sep = "_"), paste(params$date_start, params$date_end, sep = "_"), ".csv", sep="_"), sep="/"))


```



# older Nooksack code 
```{r}

all_catch_by_angler_type <- int4 |> 
  mutate_at("Catch_Group", str_replace, "Chinook_", "") |> 
  group_by(angler_type, Catch_Group) |> 
  summarise(
    Total_Catch_All_Days = sum(Total_Catch_Expanded),
    Variance = sum(Variance_Total_Catch_Expanded),
    SE_catch = sqrt(Variance),
    CV_catch = SE_catch/Total_Catch_All_Days,
    lwrCI = Total_Catch_All_Days - qt(1-(0.1/2),degfree)*(SE_catch),
    uprCI = Total_Catch_All_Days + qt(1-(0.1/2),degfree)*(SE_catch)
  ) |> 
  distinct() |> 
  arrange(Catch_Group, angler_type) |>
  mutate(across(1:6, round, 2))

write_csv(all_catch_by_angler_type, "outputs/2021_Nooksack_Spring_Chinook_Catch_Summary.csv")

```



## Angler effort
```{r effort table, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}
Total_Effort_table <- Effort_tab3 |>
  group_by(angler_type) |> 
  summarise(
    Total_effort = sum(Total_effort),
    Variance = sum(Variance),
    SE = sqrt(Variance),
    CV = SE / Total_effort,
    lwr90CI = Total_effort - qt(1-(0.1/2),17)*(Variance^0.5),
    upr90CI = Total_effort + qt(1-(0.1/2),17)*(Variance^0.5)
  ) |> 
  kbl(caption = "Total angler effort statistics in angler hours for boat and shore based fishing during the 2021 North Fork Nooksack River Spring Chinook fishery from 5/29/2021 - 7/9/2021.",digits =2) |> 
  # kable_classic(full_width = F, html_font = "Cambria") |> 
  kable_styling(bootstrap_options = c("striped", "hover"))

print(Total_Effort_table)


```


## Catch estimates
```{r effort table print, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}
all_catch_by_angler_type_table <- int4 |> 
  group_by(angler_type, Catch_Group) |> 
  summarise(
    Total_Catch_All_Days = sum(Total_Catch_Expanded),
    Variance = sum(Variance_Total_Catch_Expanded),
    SE_catch = sqrt(Variance),
    CV_catch = SE_catch/Total_Catch_All_Days,
    lwr90CI = Total_Catch_All_Days - qt(1-(0.1/2),degfree)*(SE_catch),
    upr90CI = Total_Catch_All_Days + qt(1-(0.1/2),degfree)*(SE_catch)
  ) |> 
  distinct() |> 
  arrange(Catch_Group, angler_type) |> 
  kbl(caption = "Total catch estimates for spring Chinook grouped by origin, life stage, and fate during the 2021 North Fork Nooksack River Spring Chinook fishery from 5/29/2021 - 7/9/2021.",digits =2) |> 
  # kable_classic(full_width = F, html_font = "Cambria")|> 
  kable_styling(bootstrap_options = c("striped", "hover"))
print(all_catch_by_angler_type_table)
```


```{r, include=TRUE, fig.align="center",fig.width= 14, fig.height= 8, fig.cap=c("Figure 1. Total catch estimates for spring Chinook grouped by origin, life stage, and fate during the 2021 North Fork Nooksack River Spring Chinook fishery from 5/29/2021 - 7/9/2021. Error bars show 90 percent confidence intervals."),echo=FALSE,warning = FALSE}

all_catch_by_angler_type |> 
  filter(!grepl(c("Steelhead"),Catch_Group)) |> 
  filter(Total_Catch_All_Days > 0) |> 
  ggplot(aes(x = Catch_Group, y = Total_Catch_All_Days, fill = angler_type)) +
  geom_bar(position = "dodge", stat = "identity", color = "black") +
  scale_fill_manual(values=c("#999999", "#E69F00")) +
  geom_errorbar(aes(ymin=lwrCI, ymax=uprCI), width=.2,
                position=position_dodge(.9)) +
  theme(legend.text=element_text(size=8)) +
  theme_bw(base_size = 12)

```



